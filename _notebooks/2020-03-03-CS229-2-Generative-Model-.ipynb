{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models\n",
    "> \"Algorithms that tries to learn mappings directly from space input $\\\\X$ to the labels $y$, or tries to map $p(y|x)$ directly, is called **discriminative learning algorithms**\"\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [machine-learning, supervised, naive-bayes, gaussian-discriminant-analysis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms that tries to learn mappings directly from space input $\\\\X$ to the labels $y$, or tries to map $p(y|x)$ directly, is called **discriminative learning algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms that tries to learn $p(x|y)$ then tries to infer $p(y|x)$ by bayes rules, is called **generative learning algorithms**.\n",
    "\n",
    "Note: Bayes Rule:\n",
    "\n",
    "\n",
    "$$\n",
    "p(y|x) = \\frac{p(x|y)p(y)}{p(x)}\n",
    "$$\n",
    "\n",
    "but then to calculate $p(y|x)$, we don't actually need to calculate denominator\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "arg \\space max_y p(y | x) &=& arg \\space max_y \\space \\frac{p(x | y) p(y)}{p(x)}\\\\\n",
    "&=&arg \\space max_y p(x|y)p(y)\n",
    "\\end{eqnarray}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Discriminant Analysis\n",
    "\n",
    "- assume that $p(x|y)$ distributed according to **multivariate normal distributions**\n",
    "$$\n",
    "p(x;\\mu, \\Sigma) = \\frac{1}{(2\\phi)^{d/2} |\\Sigma|^{1/2}}exp \\space \\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1}(x-\\mu) \\right)\n",
    "$$\n",
    "- feature $x$ is continuous-valued random variables.\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "y &\\sim& Bernoulli(\\phi)\\\\\n",
    "x|y=0 &\\sim& \\mathcal{N}(\\mu_0, \\Sigma)\\\\\n",
    "x|y=1 &\\sim& \\mathcal{N}(\\mu_1, \\Sigma)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "### GDA and Logistic Regression\n",
    "\n",
    "GDA Model:\n",
    "\n",
    "$$\n",
    "p(y=1 | x;\\phi, \\Sigma, \\mu_0, \\mu_1) = \\frac{1}{1 + exp(-\\theta^Tx)}\n",
    "$$\n",
    "\n",
    "where $\\theta$ is appropriate function of $\\phi,\\Sigma, \\mu_0, \\mu_1$\n",
    "\n",
    "while $p(x|y)$ is multivariate gaussian then $p(y|x)$ necessarily follows a logistic function.\n",
    "\n",
    "But, if $p(y|x)$ being a logistic function does not imply $p(x|y)$ is multivariate gaussian.\n",
    "\n",
    "Thus, \n",
    ">**GDA makes stronger modeling assumptions about the data than does logistic regression**.\n",
    "\n",
    "if the assumption are correct (the data is gaussian), then GDA will find better fits to the data --> a better model, and GDA is asymptotically efficient.\n",
    "\n",
    "or\n",
    "\n",
    "if the training sets is very large, then there is no algorithm is strictly better than GDA.\n",
    "\n",
    "\n",
    "**However,**\n",
    "by making significantly weaker assumptions, logistic regression is more robust and less sensitive to incorrect modeling assumptions. For example if $x|y = 0 \\sim \\text{Poisson}(\\lambda_0)$ and $x|y=1 \\sim \\text{Poisson}{lambda_1}$, then $p(y|x)$ will be logistic, but we won't get good result by fitting GDA.\n",
    "\n",
    "Summary\n",
    "> GDA makes stonger modeling assumptions, and is more data efficient **when the modeling assumptions are correct or at least approximately correct**. Logistic regression makes weaker assumptions, and **is significantly more robust to deviations from modeling assumptions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "GDA could be used if the feature vector $x$ is continuous, real valued vectors. What about discrete-valued? we will use classifying spam and ham example from lecture notes week 2 on CS229 by Andrew Ng.\n",
    "\n",
    "to model $p(x|y)$, we will assume that $x_i$'s are conditionally independent given $y$. This is called **Naive Bayes assumption**, then the classifier is called **Naive Bayes classifier**.\n",
    "\n",
    "$$\n",
    "p(x|y) = \\prod_{j=1}^{d}p(x_j|y)\n",
    "$$\n",
    "where\n",
    "\n",
    "$j$ is the word in vocab index\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1}) = \\prod_{i=1}^{n}p(x^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "maximizing this w.r.t $\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1}$ gives the maximum likelihood estimates:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\phi_{j|y=1} &=& \\frac{\\sum_{i=1}^{n} 1 \\{ x_j^{(i)} = 1 \\wedge y^{(i)} = 1\\}}{\\sum_{j=1}^{n} 1 \\{ y^{(i)} = 1\\}} \\\\\n",
    "\\phi_{j|y=0} &=& \\frac{\\sum_{i=1}^{n} 1 \\{ x_j^{(i)} = 1 \\wedge y^{(i)} = 0\\}}{\\sum_{j=1}^{n} 1 \\{ y^{(i)} = 0\\}} \\\\\n",
    "\\phi_y &=& \\frac{\\sum_{i=1}^{n} 1\\{ y^{(i)} = 1\\}}{n}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "read example: $\\phi_{j|y=1}$ is the fraction of spam ($y=1$) emails in which word $j$ appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_x_given_y(x: np.array, y:np.array, condition_y:int)-> np.array:\n",
    "    \"\"\"\n",
    "    Args\n",
    "    ---\n",
    "    x: array of documents, each has been splitted per token\n",
    "    y: labels\n",
    "    condition_y: conditioned label\n",
    "    \"\"\"\n",
    "    def _combine(list_of_counters: list)-> dict:\n",
    "        \"\"\"\n",
    "        producing map of word and total count in training data\n",
    "        \"\"\"\n",
    "        result = dict()\n",
    "        for word_counter in list_of_counters:            \n",
    "            for word in word_counter:\n",
    "                if word not in result:\n",
    "                    result[word] = 0\n",
    "                result[word] += word_counter[word]\n",
    "        return result\n",
    "    \n",
    "    filtered_x = x[y==condition_y]\n",
    "    combined = _combine(list(map(lambda s: Counter(s), filtered_x)))\n",
    "    \n",
    "    words, word_counts = (\n",
    "        np.array(list(combined.keys())), \n",
    "        np.array(list(combined.values())))\n",
    "    denominator = y[y==condition_y].shape[0]\n",
    "    prob_xs_given_y = word_counts / np.float(np.sum(word_counts))\n",
    "    return words, prob_xs_given_y\n",
    "\n",
    "def phi_y(y: np.array, condition_y: int) -> float:\n",
    "    return y[y==condition_y].shape[0] / np.float(y.shape[0])\n",
    "\n",
    "def learn(x: np.array, y: np.array):\n",
    "    all_possible_y = np.unique(y)\n",
    "    p_x_given_ys = {\n",
    "        yi: dict(zip(*phi_x_given_y(x, y, yi)))\n",
    "        for yi in all_possible_y\n",
    "    }\n",
    "    p_ys = {\n",
    "        yi: phi_y(y, yi)\n",
    "        for yi in all_possible_y\n",
    "    }\n",
    "\n",
    "    return p_x_given_ys, p_ys\n",
    "\n",
    "def h_x(p_x_given_ys: dict, p_ys: dict, x) -> int:\n",
    "    words = np.unique(x)\n",
    "    p_x_given_ys_res = np.zeros(len(p_x_given_ys.keys()))\n",
    "    \n",
    "    for yi in p_x_given_ys:\n",
    "        p_x_given_y = 0\n",
    "        for word in words:\n",
    "            if word in p_x_given_ys[yi]:\n",
    "                if p_x_given_y == 0:\n",
    "                    p_x_given_y = 1\n",
    "                p_x_given_y *= p_x_given_ys[yi][word]\n",
    "        \n",
    "        p_x_given_ys_res[yi] = p_x_given_y\n",
    "        \n",
    "    p_ys_arr = np.array(list(p_ys.values()))\n",
    "    nominator = p_x_given_ys_res * p_ys_arr\n",
    "    px = np.sum(p_x_given_ys_res * p_ys_arr) #denominator\n",
    "    \n",
    "    p_y_given_x = nominator/px\n",
    "    max_i = np.argmax(p_y_given_x)\n",
    "    return max_i, p_y_given_x[max_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    'a a a b'.split(),\n",
    "    'a b a a'.split(),\n",
    "    'b b'.split(),\n",
    "    'b b a'.split()\n",
    "])\n",
    "y = np.array([\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    0\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x_given_ys, p_ys = learn(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.7894736842105263)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_x(p_x_given_ys, p_ys, 'a '.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
