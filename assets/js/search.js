
var documents = [{
    "id": 0,
    "url": "https://ariepratama.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://ariepratama.github.io/about/",
    "title": "About Me",
    "body": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. This website is powered by fastpages 1.       a blogging platform that natively supports Jupyter notebooks in addition to other formats.  &#8617;    "
    }, {
    "id": 2,
    "url": "https://ariepratama.github.io/categories/",
    "title": "Tags",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; • {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 3,
    "url": "https://ariepratama.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 4,
    "url": "https://ariepratama.github.io/2020/03/01/CS229-1-Logistic-Regression.html",
    "title": "Title",
    "body": "2020/03/01 -           Logistic Regression&#182;Similar with linear regression but we wanted to change the output into 0 to 1. General Formula&#182;: $$h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$$where$$g(z) = \frac{1}{1+e^{-z}}$$ note that linear regression equation is:$$z_\theta = \theta^Tx$$ Derivatives of Logistic Regression&#182;: $$\begin{eqnarray}g'(z) &amp;=&amp; \frac{d}{dz} \frac{1}{1 + e^{-z}}\\ &amp;=&amp; \frac{1}{(1+e^{-z})^2} (e^{-z}) \\ &amp;=&amp; \frac{1}{(1+e^{-z})} \dot (1 - \frac{1}{(1+e^{-z})})\\ &amp;=&amp; g(z)(1 - g(z))\end{eqnarray}$$Classification with Logistic Regression&#182;: First we can formualate the classification problem into $$\begin{eqnarray}P(y=1 | x;\theta) &amp;=&amp; h_{\theta}(x)\\P(y=0 | x;\theta) &amp;=&amp; 1 - h_{\theta}(x)\end{eqnarray}$$or more generally, $$p(y | x;\theta) = h_\theta(x)^y \space (1-y_\theta(x))^{(1-y)}$$$p(y | x;\theta)$ in human language: Given feature $x$ and some model parameter $\theta$, the probability of the data could be classified as $y$ $p(y | x;\theta)$ will also be called Likelihood $L(\theta)$. $$\begin{eqnarray}L(\theta) &amp;=&amp; p(\vec{y} | X;\theta)\\&amp;=&amp;\prod_i^n p(y^{(i)} | x^{(i)};\theta)\end{eqnarray}$$and it will be easier to maximize the log likelihood: $$\begin{eqnarray}\ell(\theta) &amp;=&amp; log \space L(\theta)\\&amp;=&amp;\sum_{i=1}^{n}y^{(i)} log \\&amp;=&amp;\sum_{i=1}^{n}y^{(i)} \text{log} h(x^{(i)}) + (1-y^{(i)}) \text{log}(1-h(x^{(i)}))\end{eqnarray}$$ what about $\theta$ update rule? suprisingly we still ends up in similar update equation $$\theta_j := \theta_j + \alpha(y^{(\theta)} - h_{\theta}(x^{(i)}))x^{(i)}$$however it is not the same algorithm, because $h_\theta(x^{(i)})$it's now called perceptron learning algorithm       import numpy as np          def h_x(theta, x):  &quot;&quot;&quot;  the dot product between weight &amp; x  &quot;&quot;&quot;  y = np. zeros(x. shape[0])  z = np. zeros(x. shape[0])  for i in range(len(y)):    z[i] = theta @ x[i]  y = 1. /(1 + np. power(np. e, -z))  return ydef update_theta(theta, y, h_x, x, alpha=0. 001):  &quot;&quot;&quot;  produces new theta  &quot;&quot;&quot;  return theta + alpha * ((y - h_x) @ x)def learn(theta, x, y, max_iter=1000):  &quot;&quot;&quot;  learn with gradient descent  &quot;&quot;&quot;  i = 0  stop = False  h_x1 = h_x(theta, x)  theta_new = np. copy(theta)    while not stop or i &lt; max_iter:    theta_new = update_theta(theta, y, h_x1, x)    if np. all(np. isclose(theta, theta_new)):      stop = True          theta = np. copy(theta_new)    h_x1 = h_x(theta, x)    i += 1  return theta_newdef accuracy(y_true, y_predicted):  &quot;&quot;&quot;  calculate the accuracy of classification  &quot;&quot;&quot;  return np. sum(y_true == y_predicted)/np. float(y_true. shape[0])          import matplotlib. pyplot as plt          np. random. seed(2)x = np. array([[1. ], [2. ], [3. ], [4. ], [5. ], [6. ]])y = np. array([0, 1, 0, 0, 1,1])theta = np. random. uniform(0, 1, 1)h_x1 = h_x(theta, x)theta_new = learn(theta, x, y)f, ax = plt. subplots(figsize=(12, 7))_x = np. linspace(-10, 10). reshape(-1, 1)plt. subplot(121)plt. title(&#39;Before Learning, theta: {:. 3f}&#39;. format(theta[0]))plt. scatter(x[:, 0], y)plt. plot(_x, h_x(theta, _x), c=&#39;orange&#39;, linestyle=&#39;--&#39;)plt. subplot(122)plt. title(&#39;After Learning, new theta: {:. 3f}&#39;. format(theta_new[0]))_x = np. linspace(-10, 10). reshape(-1, 1)plt. scatter(x[:, 0], y)plt. plot(_x, h_x(theta_new, _x), c=&#39;orange&#39;, linestyle=&#39;--&#39;)  [&lt;matplotlib. lines. Line2D at 0x7fb5539e8850&gt;]  "
    }, {
    "id": 5,
    "url": "https://ariepratama.github.io/2020/03/01/CS229-1-Linear-Regression.html",
    "title": "Title",
    "body": "2020/03/01 -           Linear Regression&#182;General Form&#182;: $$h(x) = \sum_{i=0}^{d} \theta_i x_i = \theta^{T}x$$where $\theta$ is the parameters $x$ is the variable Cost Function&#182;: $$J(\theta) = \frac{1}{2} \sum_{i=1}^{n} (h_{\theta}(x^{(i)}) - y^{(i)})^{2}$$Regresssion with this kind of loss function is called ordinary least squares LMS (Least Mean Squared) Algorithm&#182;: We want to choose $\theta$ to minimize $J(\theta)$to do so, we start with initial guessrepeatedly update $\theta$ to make $J(\theta)$ smaller$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta)$$where $\alpha$ is the learning rate since the derivatives $\frac{\partial}{\partial\theta_j} J(\theta)$ is $$\frac{\partial}{\partial\theta_j} J(\theta) = (h_\theta(x) - y)x_j$$then we will get the update rule for $\theta_j$ $$\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x^{(i)}$$      import pandas as pdimport numpy as np          def h_x(theta, x):  &quot;&quot;&quot;  the dot product between weight &amp; x  &quot;&quot;&quot;  y = np. zeros(x. shape[0])    for i in range(len(y)):    y[i] = theta @ x[i]  return ydef update_theta(theta, y, h_x, x, alpha=0. 01):  &quot;&quot;&quot;  produces new theta  &quot;&quot;&quot;  return theta + alpha * ((y - h_x) @ x)          def learn(theta, x, y, max_iter=1000):  &quot;&quot;&quot;  learn with gradient descent  &quot;&quot;&quot;  i = 0  stop = False  h_x1 = h_x(theta, x)  theta_new = np. copy(theta)    while not stop or i == max_iter:    theta_new = update_theta(theta, y, h_x1, x)          if np. all(np. isclose(theta, theta_new)):      stop = True          theta = np. copy(theta_new)    h_x1 = h_x(theta, x)    i += 1  return thetadef rmse(y_true, y_predicted):  &quot;&quot;&quot;  calculate root mean squared error  &quot;&quot;&quot;  return np. mean(np. sqrt(np. power(y_predicted - y_true, 2)))          x = np. array([[1. ,1. ,3. ],[2. ,2. ,2. ],[3. ,3. ,3. ], [4. , 4. , 4. ]])y = np. array([2. ,4. ,6. , 8. ])theta = np. array([1. ,1. ,1. ])pred = h_x(theta, x)print(&#39;initial prediction: {}, rmse: {}&#39;. format(pred, rmse(y, pred)))print(&#39;y: {}&#39;. format(y))new_theta = learn(theta, x, y)pred = h_x(new_theta, x)print(&#39;current_prediction: {}, rmse: {}&#39;. format(pred, rmse(y, pred)))  initial prediction: [ 5.  6.  9. 12. ], rmse: 3. 0y: [2. 4. 6. 8. ]current_prediction: [2. 00000075 3. 99999991 5. 99999987 7. 99999982], rmse: 2. 8862683865149563e-07  this algorithm is called stochastic gradient descent (we just need to change theta into some random numbers instead of starting from 1 for each dimension) or incremental gradient descent       import seaborn as snsimport matplotlib. pyplot as pltplt. style. use(&#39;ggplot&#39;)          np. random. seed(3)theta = np. random. rand(1)x = np. array([[1. ],[2. ],[3. ]])y = np. array([2. ,4. ,6. ])theta_new = learn(theta, x, y)pred = h_x(theta_new, x)f, ax = plt. subplots(figsize=(12, 7))plt. subplot(121)plt. title(&#39;Before Learning&#39;)plt. scatter(x[:,0], y)plt. plot(x, h_x(theta, x), linestyle=&#39;--&#39;, c=&#39;orange&#39;)plt. subplot(122)plt. title(&#39;After Learning&#39;)plt. scatter(x=x[:,0], y=y)plt. plot(x, pred, linestyle=&#39;--&#39;, c=&#39;orange&#39;)  [&lt;matplotlib. lines. Line2D at 0x7ff28c420390&gt;]  References&#182;Stanford CS229 Lecture Notes. http://tutorial. math. lamar. edu/pdf/calculus_cheat_sheet_derivatives. pdf"
    }, {
    "id": 6,
    "url": "https://ariepratama.github.io/fastpages/jupyter/2020/02/20/test.html",
    "title": "Fastpages Notebook Blog Post",
    "body": "2020/02/20 -           About&#182;This notebook is a demonstration of some of capabilities of fastpages with notebooks. With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! Front Matter&#182;: Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: Setting toc: true will automatically generate a table of contentsSetting badges: true will automatically include GitHub and Google Colab links to your notebook. Setting comments: true will enable commenting on your blog post, powered by utterances. More details and options for front matter can be viewed on the front matter section of the README. Markdown Shortcuts&#182;: put a #hide flag at the top of any cell you want to completely hide in the docs put a #collapse flag at the top of any cell if you want to hide that cell by default, but stil have it be visible to the reader:              #collapseimport pandas as pdimport altair as alt       put a #collapse_show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:              #collapse_showcars = &#39;https://vega. github. io/vega-datasets/data/cars. json&#39;movies = &#39;https://vega. github. io/vega-datasets/data/movies. json&#39;sp500 = &#39;https://vega. github. io/vega-datasets/data/sp500. csv&#39;stocks = &#39;https://vega. github. io/vega-datasets/data/stocks. csv&#39;flights = &#39;https://vega. github. io/vega-datasets/data/flights-5k. json&#39;       Interactive Charts With Altair&#182;: Charts made with Altair remain interactive.  Example charts taken from this repo, specifically this notebook. Example 1: DropDown&#182;:       # single-value selection over [Major_Genre, MPAA_Rating] pairs# use specific hard-wired values as the initial selected valuesselection = alt. selection_single(  name=&#39;Select&#39;,  fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;],  init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;},  bind={&#39;Major_Genre&#39;: alt. binding_select(options=genres), &#39;MPAA_Rating&#39;: alt. binding_radio(options=mpaa)}) # scatter plot, modify opacity based on selectionalt. Chart(movies). mark_circle(). add_selection(  selection). encode(  x=&#39;Rotten_Tomatoes_Rating:Q&#39;,  y=&#39;IMDB_Rating:Q&#39;,  tooltip=&#39;Title:N&#39;,  opacity=alt. condition(selection, alt. value(0. 75), alt. value(0. 05)))    Example 2: Tooltips&#182;:       alt. Chart(movies). mark_circle(). add_selection(  alt. selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;])). encode(  x=&#39;Rotten_Tomatoes_Rating:Q&#39;,  y=alt. Y(&#39;IMDB_Rating:Q&#39;, axis=alt. Axis(minExtent=30)), # use min extent to stabilize axis title placement  tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;]). properties(  width=600,  height=400)    Example 3: More Tooltips&#182;:       # select a point for which to provide details-on-demandlabel = alt. selection_single(  encodings=[&#39;x&#39;], # limit selection to x-axis value  on=&#39;mouseover&#39;, # select on mouseover events  nearest=True,  # select data point nearest the cursor  empty=&#39;none&#39;   # empty selection includes no data points)# define our base line chart of stock pricesbase = alt. Chart(). mark_line(). encode(  alt. X(&#39;date:T&#39;),  alt. Y(&#39;price:Q&#39;, scale=alt. Scale(type=&#39;log&#39;)),  alt. Color(&#39;symbol:N&#39;))alt. layer(  base, # base line chart    # add a rule mark to serve as a guide line  alt. Chart(). mark_rule(color=&#39;#aaa&#39;). encode(    x=&#39;date:T&#39;  ). transform_filter(label),    # add circle marks for selected time points, hide unselected points  base. mark_circle(). encode(    opacity=alt. condition(label, alt. value(1), alt. value(0))  ). add_selection(label),  # add white stroked text to provide a legible background for labels  base. mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2). encode(    text=&#39;price:Q&#39;  ). transform_filter(label),  # add text labels for stock prices  base. mark_text(align=&#39;left&#39;, dx=5, dy=-5). encode(    text=&#39;price:Q&#39;  ). transform_filter(label),    data=stocks). properties(  width=700,  height=400)    Data Tables&#182;: You can display tables per the usual way in your blog:       movies = &#39;https://vega. github. io/vega-datasets/data/movies. json&#39;df = pd. read_json(movies)# display table with pandasdf[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;,   &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]]. head()           Title   Worldwide_Gross   Production_Budget   IMDB_Rating         0   The Land Girls   146083. 0   8000000. 0   6. 1       1   First Love, Last Rites   10876. 0   300000. 0   6. 9       2   I Married a Strange Person   203134. 0   250000. 0   6. 8       3   Let's Talk About Sex   373615. 0   300000. 0   NaN       4   Slam   1087521. 0   1000000. 0   3. 4     Images&#182;: Local Images&#182;: You can reference local images and they will be copied and rendered on your blog automatically.  You can include these with the following markdown syntax: ![](my_icons/fastai_logo. png) Remote Images&#182;: Remote images can be included with the following markdown syntax: ![](https://image. flaticon. com/icons/svg/36/36686. svg) Animated Gifs&#182;: Animated Gifs work, too! ![](https://upload. wikimedia. org/wikipedia/commons/7/71/ChessPawnSpecialMoves. gif) Captions&#182;: You can include captions with markdown images like this: ![](https://www. fast. ai/images/fastai_paper/show_batch. png  Credit: https://www. fast. ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/ ) Other Elements&#182;Tweetcards&#182;: Typing &gt; twitter: https://twitter. com/jakevdp/status/1204765621767901185?s=20 will render this:Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Youtube Videos&#182;: Typing &gt; youtube: https://youtu. be/XfoYk_Z5AkI will render this: Boxes / Callouts&#182;: Typing &gt; Warning: There will be no second warning! will render this:    Warning: There will be no second warning! Typing &gt; Important: Pay attention! It's important. will render this:    Important: Pay attention! It&#8217;s important. Typing &gt; Tip: This is my tip. will render this:    Tip: This is my tip. Typing &gt; Note: Take note of this. will render this:    Note: Take note of this. Typing &gt; Note: A doc link to [an example website: fast. ai](https://www. fast. ai/) should also work fine. will render in the docs:    Note: A doc link to an example website: fast. ai should also work fine. "
    }, {
    "id": 7,
    "url": "https://ariepratama.github.io/fastpages/markdown/2020/01/14/test-markdown-post.html",
    "title": "Example Markdown Post",
    "body": "2020/01/14 - Basic setup: Jekyll requires blog post files to be named according to the following format: YEAR-MONTH-DAY-filename. md Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. . md is the file extension for markdown files. The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. Basic formatting: You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: Lists: Here’s a list:  item 1 item 2And a numbered list:  item 1 item 2Boxes and stuff:  This is a quotation    You can include alert boxes…and…    You can include info boxesImages: Code: General preformatted text: # Do a thingdo_thing()Python code and output: # Prints '2'print(1+1)2Tables:       Column 1   Column 2         A thing   Another thing   Tweetcards: Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019Footnotes:       This is the footnote.  &#8617;    "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}